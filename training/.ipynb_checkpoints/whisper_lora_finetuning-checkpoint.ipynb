{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper LoRA Fine-tuning for Heve AI\n",
    "\n",
    "This notebook fine-tunes OpenAI Whisper using LoRA (Low-Rank Adaptation) on your collected speech data.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU runtime (T4, V100, or A100 recommended)\n",
    "- 100+ corrected transcription samples\n",
    "- Upload your `training/data/` folder to this environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft bitsandbytes accelerate librosa soundfile evaluate jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your training data\n",
    "data_dir = Path(\"training/data\")\n",
    "csv_file = data_dir / \"transcriptions.csv\"\n",
    "audio_dir = data_dir / \"audio\"\n",
    "\n",
    "# Read CSV and filter for corrected transcriptions\n",
    "df = pd.read_csv(csv_file)\n",
    "corrected_df = df[df['corrected_transcription'].notna() & (df['corrected_transcription'] != '')]\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Corrected samples: {len(corrected_df)}\")\n",
    "\n",
    "if len(corrected_df) < 10:\n",
    "    raise ValueError(\"Need at least 10 corrected samples for training\")\n",
    "\n",
    "# Prepare dataset\n",
    "training_data = []\n",
    "for _, row in corrected_df.iterrows():\n",
    "    audio_path = audio_dir / row['audio_file']\n",
    "    if audio_path.exists():\n",
    "        training_data.append({\n",
    "            'audio': str(audio_path),\n",
    "            'sentence': row['corrected_transcription']\n",
    "        })\n",
    "\n",
    "print(f\"Training samples with audio: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Split into train/validation (80/20)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Eval samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and model\n",
    "model_name = \"openai/whisper-base\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # rank\n",
    "    lora_alpha=64,  # scaling parameter\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess audio and text for training\"\"\"\n",
    "    audio = examples[\"audio\"]\n",
    "    \n",
    "    # Process audio\n",
    "    inputs = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"], \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Process text\n",
    "    labels = processor.tokenizer(\n",
    "        examples[\"sentence\"], \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=448\n",
    "    ).input_ids\n",
    "    \n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by loss\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    examples[\"input_features\"] = inputs.input_features[0]\n",
    "    examples[\"labels\"] = labels[0]\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function, \n",
    "    remove_columns=eval_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-lora-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# Evaluation metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"ðŸš€ Starting LoRA fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model()\n",
    "print(\"âœ… Training complete! Model saved to ./whisper-lora-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "test_sample = eval_dataset[0]\n",
    "input_features = test_sample[\"input_features\"].unsqueeze(0)\n",
    "\n",
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features, max_length=448)\n",
    "    \n",
    "# Decode prediction\n",
    "transcription = processor.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "print(f\"Fine-tuned prediction: {transcription}\")\n",
    "\n",
    "# Compare with original label\n",
    "original_label = processor.tokenizer.decode(test_sample[\"labels\"], skip_special_tokens=True)\n",
    "print(f\"Ground truth: {original_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Model for Local Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file with the LoRA weights\n",
    "import shutil\n",
    "\n",
    "# Zip the model directory\n",
    "shutil.make_archive(\"whisper-lora-finetuned\", \"zip\", \"./whisper-lora-finetuned\")\n",
    "\n",
    "print(\"ðŸ“¦ Model packaged as whisper-lora-finetuned.zip\")\n",
    "print(\"ðŸ’¾ Download this file and extract it to your local Heve AI directory\")\n",
    "print(\"ðŸ”§ Update src/asr.py to load the fine-tuned model:\")\n",
    "print(\"   from peft import PeftModel\")\n",
    "print(\"   base_model = WhisperModel('openai/whisper-base')\")\n",
    "print(\"   model = PeftModel.from_pretrained(base_model, './whisper-lora-finetuned')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "**Training completed!** Your personalized Whisper model is ready.\n",
    "\n",
    "**Next steps:**\n",
    "1. Download `whisper-lora-finetuned.zip`\n",
    "2. Extract to your Heve AI directory\n",
    "3. Update `src/asr.py` to use the fine-tuned model\n",
    "4. Enjoy improved accuracy for your voice and vocabulary!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
